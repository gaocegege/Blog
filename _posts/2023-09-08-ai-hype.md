---
layout: post
title: "AI 应用层的壁垒在哪里"
description: 
headline:
modified: 2023-09-08
category: genai
tags: [genai]
imagefeature:
mathjax: false
chart:
comments: true
featured: true
---

AI 最近又成为了一个火热的话题。半年前 ChatGPT 的发布就像晴天霹雳一样震惊了所有人。但半年时间已经过去，ChatGPT 的月活开始下滑，我自己在试用过不少应用后，逐渐收敛到了 Poe，GitHub Copilot 等有限的几个应用上。

正好在出差的路上没有网络，想趁机写一下我对 AI 未来的看法。就像手机和移动网络为移动互联网的发展点燃了火花，要让 AI 真正进入各行各业，我们目前仍然面临一些缺失的要素。到底缺少什么，未来 AI 应用的壁垒在哪里，这些问题有一些非常不成熟的看法，写出来抛砖引玉。

## 上一代 AI

回顾一下上一代的AI热潮，可以追溯到 12-15 年。其中一个重要的里程碑是 AlexNet 的出现。在 2012 年的 ImageNet 图像识别挑战中，AlexNet 以惊人的准确率击败了传统方法，引起了广泛的关注和兴趣。

但是，上一代的 AI 在落地应用方面面临一些挑战。在 CV（计算机视觉）和音频等领域，AI 需要更多的人工干预和专业知识才能取得良好的效果。与此同时，文本是一个广泛的应用场景，而自然语言处理（NLP）在智能化程度上确实还有提升的空间。尽管在 NLP 领域取得了一些重要进展，如机器翻译、情感分析和文本生成等，但仍然存在理解语义、处理语境和生成自然流畅文本等方面的挑战。

AI 最终在互联网搜索广告推荐中展现出了最佳的效果。这种业务不仅具有盈利能力，还能通过数据形成正反馈的飞轮效应。随着互联网应用获取到更多用户数据，推荐系统可以通过持续的训练和迭代不断提高其效果，从而为用户和广告主提供更好的服务。

而其他领域，很难具备与搜索广告推荐相同的特质。例如，CV场景需要考虑到数据安全和隐私问题，这使得以低成本形成数据和模型的正反馈循环变得困难。 

## ChatGPT

对比之前，最近的 ChatGPT 之所以能成为人类历史上增长最快的应用之一，并在文本场景中实现智能化，排除模型规模、基于对话的友好交互方式之外，最关键的是 RLHF。模型虽然大，但是如果没有进一步优化和迭代的方法，那么它的能力始终是静态的。而 AI 的想象空间，就是源于这是人工智能，是可以不断学习和优化的。

而反观现在如火如荼的 AI 应用们，确实很少有核心的壁垒。在我看来，这主要是因为不能高效低成本地利用自身获得的数据。我们看了好多 YC 投的 AIGC 应用，大部分都是在使用 ChatGPT 针对某个细分领域，利用自己对行业的理解做产品。但是这样的产品壁垒并不深，完全来自于行业的 know-how，原则上来讲不是一个 AI 公司。

现在做的比较好的应用，Perplexity AI、Midijourney、Runway 等都拥有自己的模型，并能够持续地利用新的数据迭代自己的模型。看这些项目，大部分都已经在利用互联网的规模效应，获得更多的数据，并利用新的数据进一步优化模型，提供更好的服务。已经形成了自己的飞轮效应。而如果只是依靠 claude 或者 ChatGPT 进行产品化，那将是一个非常内卷的游戏。

## In-context Learning vs. Fine-tune

那么，为什么只有少数的公司现在可以做到这样？我认为是目前在 NLP 场景下，并不存在一个方法，能够以**低成本**，利用**大量中低质量**的数据优化模型。在上一次 AI 热潮中，互联网搜索广告业务成为真正受益的行业，原因在于它能以极低的计算成本利用新的数据来优化模型。比如 TikTok，用户的行为可以被继续通过在线学习或者离线训练，来优化背后的推荐系统。

而未来的 AI 应用在各个细分领域中，壁垒也是在于数据。只有能够高效地利用数据，才能获得商业成功，避免陷入内卷的局面。利用新获得的数据不断优化模型，为用户提供更好的服务，才能在竞争中脱颖而出。因此，高效利用数据将是未来 AI 应用成功的关键。

如何利用数据，目前主要是 in-context learning 和 finetune。先来看 finetune，我认为 finetune 技术在 NLP 领域还不是非常成熟，成本高，效果难调。在图像领域，通过 lora 等方式可以以非常低的成本进行 finetune。并且 base model 是不会变化的，训练得到的 lora patch 可能只有几十 MB。这在部署的时候也会更加方便，base model 部分可以复用，只有 patch 是需要占独立显存的，是可以以很小的成本实现 finetune 和 inference，实现模型的个性化。

然而，自然语言处理领域的情况稍有不同。LLM 规模远大于 CV 领域的 SD 等模型，finetune LLM 的过拟合、灾难性遗忘等问题，都更难解决。现在如果用 qlora 或者其他算法 finetune，都有非常多的工程 trick 要处理，并且由于规模更大，对硬件资源要求很高，几十 A100 卡时已经是相对较小规模的了。

再来看 in-context learning，它的问题主要是现在不知道它的原理是什么，以至于不知道它是否能够成为 LLM 新的主流学习方式。它在未来一定会被使用的越来越多，这一点我很认可。但是它是不是能够在大量的数据下也能够继续表现出很好的效果，还是只能通过少数 trick 取得一个“还不错”的效果，我是有疑惑的。核心还是它是模型规模变大后涌现出来的能力，对它的研究都还很初级。目前 context window 毫无疑问限制了它的水平，并且通过 flash attention 获得的大 window 在工程上会遇到 context 中间部分被遗忘的问题。

综合来看，工业界还缺少高效利用数据迭代模型的能力。我认为这个能力对于 AI 来说就像是 TCP/IP 对互联网的重要性。只有能够利用数据持续优化模型，AI 才是 AI。

设想如果有了这样的“廉价 finetune”的能力，一个一个旅行代理的应用能够利用新的数据不断优化模型，将能够提供更加个性化和优化的用户体验。而 embedding 召回和 prompt engineering 等技术也是非常重要的优化方式，但是它还是在给模型添加"外脑"。通过事先设计好的召回规则和提示工程，可以引导模型生成更准确和有用的回答。但是它很难利用新的数据来迭代和优化。

## 更远的未来

这些都只是近期我们可以期待的。从更长远的时间来看，Agent 才是 AI 更值得期待的未来。如果说高效利用数据迭代模型的能力是 AI 时代的 TCP/IP，Agent 就是互联网本身。

而且，更值得（我个人）期待的是，**它会对 infra 和 developer tools 提出全新的需求**。参考互联网的发展，在早期建站对开发者工具的需求非常薄弱，一个站长用 php 加上 mysql 就能把所有的工作做完，一个 hao123 就完成了。但是随着业务的复杂度的提高和对效率的追求，互联网出现了前后端，并且出现了各自的框架。比如前端从 jquery、angular 一直迭代到 vue、react，都是在不断地提高工程师的开发效率。

而 AI 如果进入了 agent 时代，业务的复杂度会有数量级的提升。对工具的需求也会水涨船高，在 AI infra 领域也会像传统 infra 一样，出现更多的细分品类。每一个场景都需要一个好用的工具。

不过，未来什么时候来？不知道呢。

## License

- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).
- Please contact me for commercial use.
