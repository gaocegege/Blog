---
layout: post
title: "MPI，OpenMPI 与深度学习"
description: 
headline:
modified: 2020-07-07
category: 
tags: []
imagefeature:
mathjax: false
chart:
comments: true
featured: true
---

随着分布式深度学习在工业界的普及，MPI（比我的年纪还要大两岁）又迎来了新的活力。作为一个从没有在 HPC 领域有过积累的小学生，学习了许多论文与博客，还是没有理清 MPI，OpenMPI，AllReduce，ReduceScatter，RingAllReduce 等等概念之间的关系。在前段时间为了能够更好地阅读 Horovod 和 BytePS 的代码，Horovod 本身的实现并不十分复杂，但是它的部分工作其实是借助 MPI 来实现的。因此针对性地学习了一下 MPI 的相关知识，这里抛砖引玉地介绍一下 MPI 与深度学习的关系，也留作最近业余时间学习过程的记录。

对于 MPI 本身的介绍，这里引用一段 [MPI 教程介绍](https://mpitutorial.com/tutorials/mpi-introduction/zh_cn/) 中的内容：

> 在 90 年代之前，程序员可没我们这么幸运。对于不同的计算架构写并发程序是一件困难而且冗长的事情。当时，很多软件库可以帮助写并发程序，但是没有一个大家都接受的标准来做这个事情。
>
> 在当时，大多数的并发程序只出现在科学和研究的领域。最广为接受的模型就是消息传递模型。什么是消息传递模型？它其实只是指程序通过在进程间传递消息（消息可以理解成带有一些信息和数据的一个数据结构）来完成某些任务。在实践中，并发程序用这个模型去实现特别容易。举例来说，主进程（master process）可以通过对从进程（slave process）发送一个描述工作的消息来把这个工作分配给它。另一个例子就是一个并发的排序程序可以在当前进程中对当前进程可见的（我们称作本地的，locally）数据进行排序，然后把排好序的数据发送的邻居进程上面来进行合并的操作。几乎所有的并行程序可以使用消息传递模型来描述。
>
> 由于当时很多软件库都用到了这个消息传递模型，但是在定义上有些微小的差异，这些库的作者以及一些其他人为了解决这个问题就在 Supercomputing 1992 大会上定义了一个消息传递接口的标准，也就是 MPI。这个标准接口使得程序员写的并发程序可以在所有主流的并发框架中运行。并且允许他们可以使用当时已经在使用的一些流行库的特性和模型。
>
> 到 1994 年的时候，一个完整的接口标准定义好了（MPI-1）。我们要记住 MPI 只是一个接口的定义而已。然后需要程序员去根据不同的架构去实现这个接口。很幸运的是，仅仅一年之后，一个完整的 MPI 实现就已经出现了。在第一个实现之后，MPI 就被大量地使用在消息传递应用程序中，并且依然是写这类程序的标准（de-facto）。

这里十分推荐先阅读完 [MPI 教程](https://mpitutorial.com/tutorials/) 的全部内容，它是我在互联网上能找到的所有关于 MPI 的公开材料中最为深入浅出的一个教程。

简单地来理解 MPI，它是一个定义了多个原语的消息传递接口，这一接口主要被用于多进程间的通信。它的竞品包括 RPC，Distributed Shared Memory 等。关于它们的比较可以参考论文 [Message Passing, Remote Procedure Calls and Distributed Shared Memory as Communication Paradigms for Distributed Systems](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.2490&rep=rep1&type=pdf)。

MPI 的详细文档可以参考 [MPI Forum](https://www.mpi-forum.org/)，这里有 MPI 各个版本的文档（目前发布到了 3.1）。在当代的 MPI 中，接口已经有相当多。不过对我们而言最重要的只有三个部分，也对应着 [MPI 文档](https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf) 中的第三和第五章节：端到端通信，数据类型，和集合通信（Collective Communication）。

端到端通信部分主要实现了从一个进程到另一个进程的通信，核心功能由两个原语提供：

```c
int MPI_Send(const void *buf, int count, MPI_Datatype datatype, int dest,
    int tag, MPI_Comm comm)

int MPI_Recv(void *buf, int count, MPI_Datatype datatype,
    int source, int tag, MPI_Comm comm, MPI_Status *status)
```

具体内容请参考 [MPI 教程：MPI Send and Receive](https://mpitutorial.com/tutorials/mpi-send-and-receive/zh_cn/)。

集合通讯是建立在端到端通信的基础上，在一组进程内的通讯原语。其中主要包括：

```c
// Broadcasts a message from the process with rank root to all other processes of the group. 
int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype,
    int root, MPI_Comm comm)

// Gathers values from a group of processes. 
int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
    void *recvbuf, int recvcount, MPI_Datatype recvtype, int root,
    MPI_Comm comm)

// Sends data from one task to all tasks in a group. 
int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
    void *recvbuf, int recvcount, MPI_Datatype recvtype, int root,
    MPI_Comm comm)
...
```

更多请参考 [MPI 教程：广播以及集体(collective)通信](https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/zh_cn/)，[MPI 教程：MPI Scatter, Gather, and Allgather](https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/zh_cn/) 和 [MPI 文档](https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf)。

MPI 提出了这一系列为了解决进程间消息传递问题而存在的接口，但它们需要一个实现。OpenMPI 是 MPI 的常用实现之一。因此我们可以理解，MPI 是定义，是接口，而 OpenMPI 是这一接口的对应实现。这里还有一个容易混淆的概念，就是 OpenMP。OpenMP（Open Multi-Processing）与 OpenMPI，MPI 并无任何关系。它是一个针对共享内存并行编程的 API。这里特意提出，避免混淆。

而既然 OpenMPI 是 MPI 的一种实现，那针对不同的原语，采用什么算法和数据结构来实现，是实现者的自由。我们应该可以轻易地想到，针对不同的情况（主要是消息的大小等），采用不同的算法会提高整体的性能。OpenMPI 也是这样做的。

回到 AllReduce，它是 MPI 定义的一个集合通信的原语，它的语义是：

> Combines values from all processes and distributes the result back to all processes.

<figure>
	<img src="https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_allreduce_1.png" height="400" width="400">
    <figcaption>MPI_AllReduce</figcaption>
</figure>

在语义上（注意，仅限于语义的角度，在实现角度并不一定如此），MPI_AllReduce 等于 MPI_Reduce 加 MPI_Bcast。MPI_Reduce 会把结果聚集在 Root 进程，而再利用 MPI_Bcast 把 Root 进程的结果分发给所有进程，就实现了跟 MPI_AllReduce 等价的功能。

而从实现的角度，MPI_AllReduce 这一接口有非常多不同的算法实现，不同的算法在不同情景下具有不同的优劣势。在介绍算法之前，先来介绍一下 MPI_AllReduce 跟深度学习有什么关系。

要理解它们之间的关系，首先要介绍一下模型训练。在一次模型训练中，首先我们会利用数据对模型进行前向的计算。所谓的前向计算，就是将模型上一层的输出作为下一层的输入，并计算下一层的输出，从输入层一直算到输出层为止。根据目标函数，我们将反向计算模型中每个参数的导数，并且结合学习率来更新模型的参数。在分布式训练的场景中，这一问题就会更为复杂一些。比如我们利用 4 个 Worker，利用不同的数据同步地训练相同结构的模型（数据并行的同步训练），在每个 Worker 计算好梯度后，就涉及到一个梯度同步的问题。每个 Worker 都有根据自己的数据计算的梯度，如何能够让自己得到的梯度也能作用于其他的 Worker 呢？有一种方式，是引入一个中心化的组件，参数服务器。所有的参数都存储在参数服务器中，而 Worker 是万年打工仔。Worker 们只负责辛辛苦苦地计算梯度，并且把计算好的梯度发送给参数服务器。参数服务器收到梯度后，执行一定的计算（梯度平均等）后，更新其维护的参数，再把更新好的新参数返回给所有的 Worker。Worker 打工仔们会再进行下一轮的前后向计算。

<figure>
	<img src="https://pic1.zhimg.com/80/v2-ffd6d4a16b2a1fe6c5f0cf49080e8078_1440w.jpg" height="500" width="500">
    <figcaption>参数服务器更新梯度和参数（图来自参考文献）</figcaption>
</figure>

除了这样的方式之外，我们发现，MPI_AllReduce 语义也可以很好地满足这一需要。

我们可以把每个 Worker 看作是 MPI 概念中的一个进程，4 个 Worker 组成了一个 4 个进程组成的组。我们在这四个进程中对梯度进行一次 MPI_AllReduce。根据 MPI_AllReduce 的语义，所有参与计算的进程都有结果，所以梯度就完成了分发。只要在初始化的时候，我们可以保证每个 Worker 的参数是一致的，那在后续的迭代计算中，参数会一直保持一致，因为梯度信息是一致的。

<figure>
	<img src="https://pic1.zhimg.com/80/v2-68138c656ef1501ad3911a5524271960_1440w.jpg" height="500" width="500">
    <figcaption>MPI_AllReduce 方式更新梯度（图来自参考文献）</figcaption>
</figure>

所以，MPI_AllReduce 的语义可以很好地解决深度学习中梯度同步的问题。但是，到底能不能使用它，还是要看下层的实现对这一场景是否足够友好。

在 OpenMPI 的实现中，MPI_AllReduce 主要有 7 种算法，具体可以参考 [ompi/mca/coll/tuned/coll_tuned_allreduce_decision.c](https://github.com/open-mpi/ompi/blob/98afc838aa53da88cba339f6dcbab256806a5745/ompi/mca/coll/tuned/coll_tuned_allreduce_decision.c)

```c
/* valid values for coll_tuned_allreduce_forced_algorithm */
static mca_base_var_enum_value_t allreduce_algorithms[] = {
    {0, "ignore"},
    {1, "basic_linear"},
    {2, "nonoverlapping"},
    {3, "recursive_doubling"},
    {4, "ring"},
    {5, "segmented_ring"},
    {6, "rabenseifner"},
    {0, NULL}
};
```

我们可以静态地指定算法，也可以让 OpenMPI 来[决定](https://github.com/open-mpi/ompi/blob/2b1f0533451b59dc3800a26c169b0a330f329e2f/ompi/mca/coll/tuned/coll_tuned_decision_dynamic.c#L74)。当然，这不是这篇文章的重点。在深度学习这一场景下，被最为广泛应用的是 RingAllReduce 这一实现。在 OpenMPI 中，这一实现在 [ompi/mca/coll/base/coll_base_allreduce.c](https://github.com/open-mpi/ompi/blob/2ae3cfd9bc9aa8cab80986a1921fd7ad9d198d07/ompi/mca/coll/base/coll_base_allreduce.c#L277)。它的注释非常简洁明了地介绍了实现原理，建议阅读。简单来说，它利用了 MPI 的端到端通信的原语，实现了 RingAllReduce 的功能。将 RingAllReduce 分为了两个阶段。第一个阶段等价于 MPI_ReduceScatter 的语义，是将结果计算到不同的进程。第二个阶段等价于 MPI_AllGather 语义，将计算结果聚合到所有进程。

<figure>
	<img src="{{ site.url }}/images/mpi/ringallreduce.png" height="500" width="500">
    <figcaption>MPI_AllReduce Ring 实现</figcaption>
</figure>

MPI_ReduceScatter 这一接口，本身也对应着非常多的实现。如果先做一次 MPI_Reduce 再做一次 MPI_Scatter（对应 [ompi_coll_base_reduce_scatter_intra_nonoverlapping](https://github.com/open-mpi/ompi/blob/2ae3cfd9bc9aa8cab80986a1921fd7ad9d198d07/ompi/mca/coll/base/coll_base_reduce_scatter.c#L47)），性能一定无法接受。所以这里的实现使用的是 [ompi_coll_base_reduce_scatter_intra_ring](https://github.com/open-mpi/ompi/blob/2ae3cfd9bc9aa8cab80986a1921fd7ad9d198d07/ompi/mca/coll/base/coll_base_reduce_scatter.c#L397)。通过 N-1 步，我们可以实现 MPI_ReduceScatter 的语义。其中每步中每个进程的上下行通信量都是 M/N。其中个 M 是数组的长度，N 是进程的数量。数组会被分为 N 等分，所以每次通信量是 M/N。

第二个阶段，就是 MPI_AllGather 了。MPI_AllGather 本身也有非常多的算法实现。RingAllReduce 使用的是 [ompi_coll_base_allgather_intra_ring](https://github.com/open-mpi/ompi/blob/2ae3cfd9bc9aa8cab80986a1921fd7ad9d198d07/ompi/mca/coll/base/coll_base_allgather.c#L350)。这一实现一共需要 N-1 步。在第 i 步的时候，Rank r 进程会收到来自 r-1 进程的信息，这一信息中包括了 r-i-1 进程的数据。同时，r 进程会给 r+1 进程发送包含 r-i 进程的数据。所以每步中每个进程的上下行通信量同样都是 M/N。

<figure>
	<img src="{{ site.url }}/images/mpi/allgather.png" height="500" width="500">
    <figcaption>MPI_Allgather Ring 实现</figcaption>
</figure>

所以整体来看，单步中每个进程的上下行通信量为 M/N，而在整个过程中，每个进程的上下行通信量都是 2(N-1)*M/N。所以我们认为 RingAllReduce 对带宽特别友好，能很好地解决参数服务器架构中的带宽瓶颈问题。其实 MPI_AllGather 除了 Ring 之外还有很多更高效的实现，但由于 MPI_RingAllReduce 中对带宽的要求至少是 M/N，因此 ompi_coll_base_allgather_intra_ring 的实现已经完全够用，在任意时刻都占满 M/N 的上下行。

将 RingAllReduce 引入深度学习，是百度的工作，这一工作开源在 [baidu-research/tensorflow-allreduce](https://github.com/baidu-research/tensorflow-allreduce/commit/66d5b855e90b0949e9fa5cca5599fd729a70e874)。百度利用了 MPI 端到端通信的原语，重新实现了 ompi_coll_base_allgather_intra_ring 和 ompi_coll_base_reduce_scatter_intra_ring。至于不直接使用 MPI_AllReduce 的原语，猜测应该是为了兼容更多的 MPI 实现，同时避免动态选择算法导致没有启用 RingAllReduce 的可能（尽管 OpenMPI 可以静态选择算法，但可能其他实现不支持）。

百度的这一实现非常易懂，总共只有 3000 行不到的代码，其中相当部分是测试。百度提供了一个自己的 Optimizer，重载了 compute_gradients 的

```python
class DistributedOptimizer(tf.train.Optimizer):
    """An optimizer that wraps another tf.Optimizer, using an MPI allreduce to
    average gradient values before applying gradients to model weights."""

    def __init__(self, optimizer, name=None, use_locking=False):
        """Construct a new DistributedOptimizer, which uses another optimizer
        under the hood for computing single-process gradient values and
        applying gradient updates after the gradient values have been averaged
        across all the MPI ranks.
        Args:
          optimizer:
            Optimizer to use for computing gradients and applying updates.
          name:
            Optional name prefix for the operations created when applying
            gradients. Defaults to "Distributed" followed by the provided
            optimizer type.
          use_locking:
            Whether to use locking when updating variables.
            See Optimizer.__init__ for more info.
        """
        if name is None:
            name = "Distributed{}".format(type(optimizer).__name__)

        self._optimizer = optimizer
        super(DistributedOptimizer, self).__init__(
            name=name, use_locking=use_locking)

    def compute_gradients(self, *args, **kwargs):
        """Compute gradients of all trainable variables.
        See Optimizer.compute_gradients() for more info.
        In DistributedOptimizer, compute_gradients() is overriden to also
        allreduce the gradients before returning them.
        """
        gradients = (super(DistributedOptimizer, self)
                     .compute_gradients(*args, **kwargs))
        return [(allreduce(gradient), var) for (gradient, var) in gradients]
    ...

class Session(tf.Session):
    """A class for running TensorFlow operations, with copies of the same graph
    running distributed across different MPI nodes.
    The primary difference between `tf.Session` and `tf.contrib.mpi.Session` is
    that the MPI `Session` ensures that the `Session` options are correct for
    use with `tf.contrib.mpi`, and initializes MPI immediately upon the start
    of the session.
    """

    def __init__(self, target='', graph=None, config=None):
        """Creates a new TensorFlow MPI session.
        Unlike a normal `tf.Session`, an MPI Session may only use a single GPU,
        which must be specified in advance before the session is initialized.
        In addition, it only uses a single graph evaluation thread, and
        initializes MPI immediately upon starting.
        If no `graph` argument is specified when constructing the session,
        the default graph will be launched in the session. If you are
        using more than one graph (created with `tf.Graph()` in the same
        process, you will have to use different sessions for each graph,
        but each graph can be used in multiple sessions. In this case, it
        is often clearer to pass the graph to be launched explicitly to
        the session constructor.
        Args:
        target: (Optional.) The execution engine to connect to.
        graph: (Optional.) The `Graph` to be launched (described above).
        config: (Optional.) A `ConfigProto` protocol buffer with configuration
        options for the session.
        """
        super(Session, self).__init__(target, graph, config=config)

        # Initialize MPI on the relevant device.
        # TODO: Move this to library load and eliminate mpi.Session()
        self.run(init())
```

在初始化 optimizer，和使用 session 的时候，语句如下：

```python
optimizer = mpi.DistributedOptimizer(tf.train.AdamOptimizer())
with mpi.Session() as session:
```

在 optimizer 调用 compute_gradients 的时候，首先会利用 TF 自己的 optimizer 计算出本地梯度，然后利用 AllReduce 来得到各个进程平均后的梯度。而在 Session 初始化的时候会预先执行 MPI_Init 进行 MPI 环境的初始化。

在底层，AllReduce 被注册为 Op，在 ComputeAsync 中，计算请求被入队到一个队列中。这一队列会被一个统一的后台线程处理。之所以引入这样一个后台线程，在[注释](https://github.com/baidu-research/tensorflow-allreduce/commit/66d5b855e90b0949e9fa5cca5599fd729a70e874#diff-3d530d590e551619acd776cfe7eaff06R517)中有详细的介绍。

在百度的实现中，不同 Rank 的角色是不一样的，Rank 0 会充当 coordinator 的角色。它会协调来自其他 Rank 的 MPI 请求，起到一个调度协调的作用。这是一个工程上的考量，具体可以参考注释。顺便一提，百度的这个工作注释非常详尽，真乃学术界的典范。这一设计也被后来的 Horovod 采用。

Horovod 相比于百度的工作，并无学术上的贡献。但是 Horovod 扎实的工程实现，使得它受到了更多的关注。它最大的优势在于对 RingAllReduce 进行了更高层次的抽象，使其支持多种不同的框架。同时引入了 Nvidia NCCL，对 GPU 更加友好。

与百度的实现类似，Horovod 也需要先进行初始化。只不过百度把这个过程放在了 Session 构建的时候，而 Horovod 提供了显式初始化的函数。在初始化的时候，Horovod 会调用 MPI_Comm_dup 获取一个 Communicator。之所以不直接使用默认的 `MPI_COMM_WORLD`，参考这里的[文档](https://mpitutorial.com/tutorials/introduction-to-groups-and-communicators/)：

> While MPI_Comm_split is the most common communicator creation function, there are many others. MPI_Comm_dup is the most basic and creates a duplicate of a communicator. It may seem odd that there would exist a function that only creates a copy, but this is very useful for applications which use libraries to perform specialized functions, such as mathematical libraries. In these kinds of applications, it’s important that user codes and library codes do not interfere with each other. To avoid this, the first thing every application should do is to create a duplicate of MPI_COMM_WORLD, which will avoid the problem of other libraries also using MPI_COMM_WORLD. The libraries themselves should also make duplicates of MPI_COMM_WORLD to avoid the same problem.

除此之外，在初始化的时候，Horovod 还会创建一个后台线程。这里的后台线程的作用与百度的实现类似。

```C++
void horovod_init_comm(MPI_Comm comm) {
  MPI_Comm_dup(comm, &mpi_context.mpi_comm);
  InitializeHorovodOnce(nullptr, 0);
}
// Start Horovod background thread. Ensure that this is
// only done once no matter how many times this function is called.
void InitializeHorovodOnce(const int* ranks, int nranks) {
  // Ensure background thread is only started once.
  if (!horovod_global.initialize_flag.test_and_set()) {
    horovod_global.control_operation = ParseControllerOpsFromEnv();
    horovod_global.cpu_operation = ParseCPUOpsFromEnv();
#if HAVE_MPI
    // Enable mpi is it's used either in cpu data transfer or controller
    if (horovod_global.cpu_operation == LibType::MPI ||
        horovod_global.control_operation == LibType::MPI) {
      mpi_context.Enable();
    }

    if (horovod_global.control_operation == LibType::MPI){
      horovod_global.controller.reset(new MPIController(
          horovod_global.response_cache,
          horovod_global.tensor_queue, horovod_global.timeline,
          horovod_global.parameter_manager, mpi_context));
      horovod_global.controller->SetRanks(ranks, nranks);
    }
#endif
    // Reset initialization flag
    horovod_global.initialization_done = false;
    horovod_global.background_thread = std::thread(
        BackgroundThreadLoop, std::ref(horovod_global));
  }

  // Wait to ensure that the background thread has finished initializing MPI.
  while (!horovod_global.initialization_done) {
    std::this_thread::sleep_for(std::chrono::milliseconds(1));
  }
  LOG(DEBUG) << "Background thread init done";
}
```

在这个后台线程的初始化过程中，它会利用进程内共享的全局状态在自己的内存里创建一些对象，以及一些逻辑判断。比如要不要进行 Hierarchical AllReduce，要不要 AutoTune（后面会详细介绍）等。在创建的对象中，比较重要的是 op_manager，它是对真正计算的抽象。

```c++
class OperationManager {
public:
  OperationManager(ParameterManager* param_manager,
                   std::vector<std::shared_ptr<AllreduceOp>> allreduce_ops,
                   std::vector<std::shared_ptr<AllgatherOp>> allgather_ops,
                   std::vector<std::shared_ptr<BroadcastOp>> broadcast_ops,
                   std::shared_ptr<JoinOp> join_op,
                   std::vector<std::shared_ptr<AllreduceOp>> adasum_ops,
                   std::shared_ptr<ErrorOp> error_op);

  virtual ~OperationManager() = default;

  Status ExecuteAllreduce(std::vector<TensorTableEntry>& entries, const Response& response) const;

  Status ExecuteAllgather(std::vector<TensorTableEntry>& entries, const Response& response) const;

  Status ExecuteBroadcast(std::vector<TensorTableEntry>& entries, const Response& response) const;

  Status ExecuteError(std::vector<TensorTableEntry>& entries, const Response& response) const;

  Status ExecuteJoin(std::vector<TensorTableEntry>& entries, const Response& response) const;

  Status ExecuteAdasum(std::vector<TensorTableEntry>& entries, const Response& response) const;

  Status ExecuteOperation(std::vector<TensorTableEntry>& entries, const Response& response) const;
}
```

## 参考文献

- [分布式训练的方案和效率对比](https://zhuanlan.zhihu.com/p/50116885)

## License

- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).
- Please contact me for commercial use.
