---
layout: post
title: "云原生自动机器学习系统 kubeflow/katib 的设计与实现"
description: 
headline:
modified: 2019-08-05
category: 机器学习
tags: []
imagefeature:
mathjax: false
chart:
comments: true
featured: true
---

随着 Kubernetes 的发展，有越来越多的公司尝试利用 Kubernetes 来管理机器学习工作负载，[Kubeflow][] 也受到了更多的关注。社区中的各种项目都有了更多的贡献者和用户。[katib][] 作为社区中云原生的自动机器学习系统，在近期发布了新的版本。这一文章主要介绍新版本 v1alpha2 的设计与实现。[katib][] 第一个版本的介绍可以参考文章 [Katib: Kubernetes native 的超参数训练系统](http://gaocegege.com/Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/katib)。

## 问题背景

如今，机器学习技术和算法几乎应用于每个领域。而建立一个高质量的机器学习模型是一个迭代的、复杂的、耗时的过程，除了要有一个有效调整超参数的良好经验外，还需要尝试不同的算法和技术。有效地进行这个过程需要扎实的知识和经验。随着数据量的持续和巨大增长，人们已经认识到，有知识的数据科学家的数量无法扩大到足以解决这些难题。因此，将建立良好机器学习模型的过程自动化是至关重要的。

目前，深度学习领域有超参数搜索和模型结构搜索等技术来用计算力来减少人力需求。这些技术的主要目的是通过扮演领域专家的角色，减少人工在该过程中的作用，填补非专家机器学习用户的空白。这篇文章主要介绍了云原生的超参数搜索与模型结构搜索系统 [katib][] 的设计与实现，并且对比了与其他工作的优劣。

## 相关工作

### 超参数搜索

超参数训练在工业界和学术界都有相当程度的关注，目前也已经相对成熟。其问题可以被形式化地表示为在给定的多维搜索空间下的黑盒优化问题。对于单一的超参数而言，其可能是离散的值，如激活函数的选择等，也有可能是连续的值。而由于超参数的数量不确定，因此搜索空间是多维的。而由于对于超参数训练而言，输入的参数与输出是相互独立的变量，因此属于黑盒优化的范畴。

目前工业界较常用的超参数训练算法主要有随机搜索，网格搜索和贝叶斯优化等。

网格搜索是指在所有候选的参数选择中，通过循环遍历尝试每一种可能性，表现最好的参数就是最终的结果。网格搜索算法思路及实现方式都很简单，但经过笛卡尔积组合后会扩大搜索空间，并且在存在某种不重要的超参数的情况下，网格搜索会浪费大量的时间及空间做无用功，因此它只适用于超参数数量小的情况。

针对网格搜索的不足，Bengio 等人提出了随机搜索方法。随机搜索首先为每类超参数定义一个边缘分布，通常取均匀分布，然后在这些参数上采样进行搜索。

随机搜索虽然有随机因素导致搜索结果可能特别差，但是也可能效果特别好。总体来说效率比网格搜索更高，但是不保证一定能找到比较好的超参数。

贝叶斯优化是一个经典的黑盒优化算法，其利用了无限维的高斯过程来模拟黑盒的超参数搜索的目标函数形式，限于篇幅，不再赘述，其大致原理是利用之前探索的结果，探索更有可能的新的参数选择。

除此之外，学术界还有许多其他的超参数搜索算法，如 HyperBand 等。

### 模型结构搜索

模型结构搜索是目前学术界的研究热点之一。目前阶段，神经网络的结构设计需要特定领域的算法科学家进行人工的分析与建模，而模型结构搜索希望通过强化学习或者进化算法的方式，自动地寻找到最优的模型结构。

目前比较有影响力的工作有 ENAS，Auto-Keras，DARTS，ProxylessNAS 等。目前的 NAS 都会采用某种意义上的参数共享，来加速模型搜索的过程，进而缓解其对算力的巨大要求。

### 自动机器学习系统

超参数搜索与模型结构搜索，一定程度上，是可通用的算法。因此目前也有许多自动机器学习系统，帮助算法科学家们更方便地进行自动机器学习。其中 Google Vizier 对超参数搜索问题进行了非常好的抽象，并且基于此实现了谷歌内部的超参数搜索系统。

<figure>
	<img src="{{ site.url }}/images/katib-new/1.png" height="500" width="500">
    <figcaption>Google Vizier 架构图</figcaption>
</figure>

受这一工作启发，工业界有许多开源实现，其中最具代表性的是 [advisor][]。

[auto-keras][] 是一个专门用来进行模型结构搜索的库。它提供了模型结构搜索的基准实现，其中包括随机搜索，网格搜索，贪心算法和贝叶斯优化等。与此同时，它允许用户自行定义新的模型结构搜索算法，并且集成在 [auto-keras][] 中。

除此之外，[nni][] 是由微软亚研院开源的自动机器学习工具。它是既支持模型结构搜索，也支持超参数搜索的工具。它为模型结构搜索实现了一个[统一的接口](https://github.com/microsoft/nni/blob/master/docs/en_US/AdvancedFeature/GeneralNasInterfaces.md)，相比于其他的工具，具有更好的易用性。

## katib 的设计与实现

虽然目前已经有了许多可以帮助算法工程师们快速进行参数搜索的工具，但目前的工具，大多会引入额外的组件。这样的实现会增加运维的负担。[katib][] 旨在实现一个云原生的超参数搜索与模型结构搜索系统，复用 Kubernetes 对 GPU 等资源的管理能力，同时保证系统的可扩展性。云原生的实现使得维护这一系统的工作对运维工程师更加友好。

### 目前的设计

为了实现 [katib][] 云原生的架构目标，新的版本引入了两个 CRD：Experiment 和 Trial。Experiment 定义类似于 Google Vizier 中的 Study，在 [katib][] v1alpha1 中，它的名字是 StudyJob。它存储了超参数搜索或者模型结构搜索的搜索空间，以及 Trial 训练任务的模板，和优化的目标值等内容。Trial 是 Experiment 在确定了搜索空间中的一组参数取值后，进行的模型训练任务。它包含训练任务实例，以及训练的指标等。当 Trial 的状态为 Completed 时，就意味着这一组参数被验证了，而且对应的训练指标被收集起来了。

而 Job 是 Kubernetes 上的一个训练实例，它是用来验证一个 Trial 中的参数的。Job 可以是 TensorFlow 模型训练任务 [TFJob](https://github.com/kubeflow/tf-operator)，也可以是 PyTorch 训练任务 [PyTorchJob](https://github.com/kubeflow/pytorch-operator)，也可以是以 Kubernetes batchv1/job 运行的任意框架的训练代码。

Suggestion 是在 Experiment 中定义的搜索空间中寻找一组参数取值的算法。寻找到的参数取值会交给 Trial 去运行。目前 kaitb 支持：

* random search
* grid search
* [hyperband](https://arxiv.org/pdf/1603.06560.pdf)
* [bayesian optimization](https://arxiv.org/pdf/1012.2599.pdf)
* [NAS based on reinforcement learning](https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1alpha1/NAS_Reinforcement_Learning)
* [NAS based on EnvelopeNets](https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1alpha1/NAS_Envelopenet)

#### 例子

接下来，我们以一次模型训练为例，从用户的角度看下 katib 的使用流程。

对于用户而言，他/她只需要创建出 Experiment 即可进行超参数搜索训练：

```yaml
apiVersion: "kubeflow.org/v1alpha2"
kind: Experiment
metadata:
  namespace: kubeflow
  name: random-experiment
spec:
  parallelTrialCount: 3
  maxTrialCount: 12
  maxFailedTrialCount: 3
  objective:
    type: maximize
    goal: 0.99
    objectiveMetricName: Validation-accuracy
    additionalMetricNames:
      - accuracy
  algorithm:
    algorithmName: random
  trialTemplate:
    goTemplate:
        rawTemplate: |-
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: {{.Trial}}
            namespace: {{.NameSpace}}
          spec:
            template:
              spec:
                containers:
                - name: {{.Trial}}
                  image: katib/mxnet-mnist-example
                  command:
                  - "python"
                  - "/mxnet/example/image-classification/train_mnist.py"
                  - "--batch-size=64"
                  {{- with .HyperParameters}}
                  {{- range .}}
                  - "{{.Name}}={{.Value}}"
                  {{- end}}
                  {{- end}}
                restartPolicy: Never
  parameters:
    - name: --lr
      parameterType: double
      feasibleSpace:
        min: "0.01"
        max: "0.03"
    - name: --num-layers
      parameterType: int
      feasibleSpace:
        min: "2"
        max: "5"
    - name: --optimizer
      parameterType: categorical
      feasibleSpace:
        list:
        - sgd
        - adam
        - ftrl
```

这一个配置包含了完成一个超参数训练需要的所有信息，其中包括需要优化的目标指标，需要用到的搜索算法，Trial 的训练代码，以及搜索空间等。下面我们将会一个个介绍。

```yaml
  parallelTrialCount: 3
  maxTrialCount: 12
  maxFailedTrialCount: 3
```

首先是关于并发相关的配置。其中的 `parallelTrialCount` 是定义了允许的并行数，在这个例子中，用户允许同一时间内有 3 个运行的 Trial。`maxTrialCount` 是最大的 Trial 数量，在例子中为 12，也就是在 12 个 Trial 运行完后，Experiment 就被标记为完成。`maxFailedTrialCount` 是最大允许失败的次数，在这一例子中，用户可以容忍 3 个 Trial 是训练失败的。

```yaml
  objective:
    type: maximize
    goal: 0.99
    objectiveMetricName: Validation-accuracy
    additionalMetricNames:
      - accuracy
```

`objective` 这一部分是对训练目标的定义。在这一例子中，用户定义了训练的目标是准确率，训练目标是最大化准确率，当准确率到达 0.99 时，超参数训练任务可以被认为已经完成。

```yaml
  algorithm:
    algorithmName: random
  trialTemplate:
    goTemplate:
        rawTemplate: |-
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: {{.Trial}}
            namespace: {{.NameSpace}}
          spec:
            template:
              spec:
                containers:
                - name: {{.Trial}}
                  image: katib/mxnet-mnist-example
                  command:
                  - "python"
                  - "/mxnet/example/image-classification/train_mnist.py"
                  - "--batch-size=64"
                  {{- with .HyperParameters}}
                  {{- range .}}
                  - "{{.Name}}={{.Value}}"
                  {{- end}}
                  {{- end}}
                restartPolicy: Never
```

`algorithm` 一项定义了超参数搜索的算法，这里用户希望使用随机搜索。而后面的 `trialTemplate` 则是定义了 Trial 进行训练时需要的代码。在这一例子中，用户使用了 batch/v1 Job 类型进行训练，使用的框架是 MXNet。在其中我们可以看到一个模板的定义 `.HyperParameters`，这一定义会在确定了参数取值后被解释执行，生成完整的训练代码定义。

```yaml
  parameters:
    - name: --lr
      parameterType: double
      feasibleSpace:
        min: "0.01"
        max: "0.03"
    - name: --num-layers
      parameterType: int
      feasibleSpace:
        min: "2"
        max: "5"
    - name: --optimizer
      parameterType: categorical
      feasibleSpace:
        list:
        - sgd
        - adam
        - ftrl
```

最后，用户定义了需要搜索的参数空间。在这个例子中，用户一共定义了三个需要搜索的参数，分别是 `--lr`，`--num-layers` 和 `--optimizer`。其中前面的两个参数有 2 种选择，最后的 opmitizer 则有 sgd，adam 和 ftrl 3 种选择。所以搜索空间的大小为 `2 x 2 x 3 = 12` 种选择。

### 未来规划

## 总结与分析

## 参考文献

- Elshawi, Radwa, Mohamed Maher, and Sherif Sakr. "Automated Machine Learning: State-of-The-Art and Open Challenges." arXiv preprint arXiv:1906.02287 (2019).
- Bergstra, James, and Yoshua Bengio. "Random search for hyper-parameter optimization." Journal of Machine Learning Research 13.Feb (2012): 281-305.‏
- Pelikan, Martin, David E. Goldberg, and Erick Cantú-Paz. "BOA: The Bayesian optimization algorithm." Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation-Volume 1. Morgan Kaufmann Publishers Inc., 1999.
- Hyperparameter tuning for machine learning models.
- Pham, Hieu, et al. "Efficient neural architecture search via parameter sharing." arXiv preprint arXiv:1802.03268 (2018).
- Jin, Haifeng, Qingquan Song, and Xia Hu. "Efficient neural architecture search with network morphism." arXiv preprint arXiv:1806.10282 (2018).
- Liu, Hanxiao, Karen Simonyan, and Yiming Yang. "Darts: Differentiable architecture search." arXiv preprint arXiv:1806.09055 (2018).
- Cai, Han, Ligeng Zhu, and Song Han. "Proxylessnas: Direct neural architecture search on target task and hardware." arXiv preprint arXiv:1812.00332 (2018).

## License

- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).
- Please contact me for commercial use.

[Kubeflow]: https://github.com/kubeflow/kubeflow
[@caicloud]: https://github.com/caicloud
[katib]: https://github.com/kubeflow/katib
[advisor]: https://github.com/tobegit3hub/advisor
[auto-keras]: https://autokeras.com/
[nni]: https://github.com/microsoft/nni
