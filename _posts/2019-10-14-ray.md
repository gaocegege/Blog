---
layout: post
title: "强化学习框架 Ray 在 Kubernetes 上的自动伸缩设计与实现"
description: 
headline:
modified: 2019-10-14
category: 机器学习
tags: []
imagefeature:
mathjax: false
chart:
comments: true
featured: true
---

## 背景

强化学习，特别是深度强化学习，在近年来取得了令人瞩目的进展。除了应用于模拟器和游戏领域，在工业领域也正取得长足的进步。[Ray][] 是一个为了强化学习或者类似的场景设计的机器学习框架，在最近，[Ray][] 合并了在 Kubernetes 上实现 Ray 集群自动伸缩的[代码请求](https://github.com/ray-project/ray/pull/5492)。因此，我们希望在本文中介绍这一新特性，以及上游社区采取的设计方案和其中的考量。

## 相关知识与工作

### 强化学习

在正文之前，首先简单介绍下 Ray 面向的场景：强化学习。强化学习是机器学习方法中的一种，这一问题可以被抽象为代理与环境之间的互动关系。环境是代理所处的外部环境，会与代理产生交互。在每次迭代时，代理会观察到部分或者全部的环境，然后决定采取某种行动，而采取的行动又会对环境造成影响。不同的行动会收到来自环境的不同反馈（Reward），而代理的目标就是最大化累积反馈（Return）\[1]。

其中，代理可以采取的行动的空间（Action Space）可能是离散的，如围棋等。也可能是连续的。而不少的强化学习算法只能支持连续的空间或者离散的空间。

在采取行动时，代理会根据某种策略（Policy）选择行动。策略可以是确定性的，也可以是带有随机性的。在深度强化学习中，策略会是参数化的，即策略的输出是输入是一组参数的函数（参数比如神经网络的权重和 bias）

强化学习领域有一个非常生动的例子：游戏 Flappy Bird 的 AI \[2]。在这一个例子中，代理就是玩家控制的小鸟，而环境就是充满了管道的飞行环境。小鸟的行动空间只有两个动作：向上飞或者什么都不做，原地下坠。同时小鸟的目标就是不断续命，飞行下去。这一问题可以被很好地用强化学习的方法建模解决。具体可见参考文献。

## 参考文献

\[1]: [Part 1: Key Concepts in RL - OpenAI](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)
\[2]: [带你走进强化学习－Flappy Bird游戏自学习 - 知乎文章](https://zhuanlan.zhihu.com/p/38308513)

## License

- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).
- Please contact me for commercial use.

[Ray]: https://ray.readthedocs.io/en/latest/
